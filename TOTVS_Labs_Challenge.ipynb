{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "The dataset can be used to:\n",
    "1. Create a recommendation system that suggests products to a customer based on his previous orders.\n",
    "2. Create a recommendation system that suggests the price of a new product being registered in the system.\n",
    "3. Customer churn prediction.\n",
    "4. Develop a system that forecasts the revenue of a company based on the price of its products and the quantity of sales in last months.\n",
    "5. Develope a recommendation system that suggests new sales channels to a new businessman of a segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "I have chosen the use case #3. By building a statistical model which is interpretable enough to not only predict customer churn, but also to expose the reasons that are likely to influence the customer to churn or stay, would provide information to the business to strengthen the actions that are retaining the customer and create new ones in order to overcome the weaknesses that are causing them to churn. Consequently, it will generate a reduction in the revenue loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "The source code is avaible [here](https://github.com/TiagoMHCSantana/totvs-labs-challenge/blob/master/challenge.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #v1.16.2\n",
    "import pandas as pd #v0.24.2\n",
    "from sklearn.impute import SimpleImputer #v0.20.3\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV #v0.20.3\n",
    "from sklearn.metrics import cohen_kappa_score, classification_report #v0.20.3\n",
    "import xgboost as xgb #v0.80\n",
    "import scipy.special as scp #v1.2.1\n",
    "import urllib2 as url #Python standard library - python v2.7.16\n",
    "from zipfile import ZipFile #Python standard library - python v2.7.16\n",
    "from StringIO import StringIO #Python standard library - python v2.7.16\n",
    "import datetime #Python standard library - python v2.7.16\n",
    "import time #Python standard library - python v2.7.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Functions to Extract Features\n",
    "The first function is used to transform the register date of the orders into the number of days since that date. It is easier to handle a numeric value than a datetime object, while the ordinal nature of the dates is kept.\n",
    "\n",
    "The 'compute_features(df)' function calculates and extracts some features for each customer, including the categorical features, like 'segment_code' and 'group_code', and the numerical ones, which are all computed from the attibutes 'quantity', 'total_price' and 'unit_price' of the dataset and from the attribute 'days_since_register_date' computed by the first function.\n",
    "\n",
    "Also the dependent variable (the attribute 'is_churn') is mapped for each customer and returned by the second function. Later on, it will be separated from the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_days_since_register_date(df, column):\n",
    "    df['days_since_'+column] = df[column].apply(lambda x: (datetime.datetime.now() - x).days)\n",
    "\n",
    "def compute_features(df):\n",
    "    #Create a new Data Frame in order to keep the calculated features.\n",
    "    X = pd.DataFrame()\n",
    "\n",
    "    #These columns are the same in all register of the same customer, so mean is applied just to group them.\n",
    "    #Except unit_price, which is the price of a item bought by the customer.\n",
    "    X[['is_churn', 'segment_code', 'group_code', 'average_unit_price']] =\\\n",
    "    df.groupby('customer_code').mean().reset_index()[['is_churn', 'segment_code', 'group_code', 'unit_price']]\n",
    "\n",
    "    #Calculate the average quantity of items by order and total price of orders for each customer.\n",
    "    X[['average_quantity_by_order', 'average_total_price_by_order']] =\\\n",
    "    df.groupby(['order_id', 'customer_code']).sum().groupby(level=1).mean().reset_index()[['quantity', 'total_price']]\n",
    "\n",
    "    #Calculate the most expensive item bought by each customer.\n",
    "    X['max_unit_price'] = df.groupby('customer_code').max().reset_index()['unit_price']\n",
    "\n",
    "    #Calculate the greatest number of items and the highest price paid by each customer in a single order.\n",
    "    X[['max_quantity_by_order', 'max_total_price_by_order']] =\\\n",
    "    df.groupby(['order_id', 'customer_code']).sum().groupby(level=1).max().reset_index()[['quantity', 'total_price']]\n",
    "\n",
    "    #Calculate the number of orders by customer.\n",
    "    X['total_orders'] = df.groupby(['customer_code', 'order_id']).mean().reset_index().groupby('customer_code').size()\n",
    "\n",
    "    #Calculate the number of days since last order.\n",
    "    X['days_since_last_order'] = df.groupby('customer_code').min().reset_index()['days_since_register_date']\n",
    "\n",
    "    #Calculate the number of days since the first order.\n",
    "    X['days_since_first_order'] = df.groupby('customer_code').max().reset_index()['days_since_register_date']\n",
    "\n",
    "    #Calculate the frequency of orders registered by each customer expressed as the number of orders divided by the\n",
    "    #\ttime interval from the first to the last order.\n",
    "    #Multiply by 100 just to avoid precision issues.\n",
    "    X['order_frequency'] = X['total_orders'] / (X['days_since_first_order'] - X['days_since_last_order'] + 1) * 100\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Reading\n",
    "The dataset is read directly from the repository into a memory buffer and, then, unzipped. Using Pandas, the json file is read and parsed to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   branch_id  customer_code  group_code  is_churn  item_code  \\\n",
      "0          0            143           0       0.0        854   \n",
      "1          0            433           0       0.0        246   \n",
      "2          0            486           0       0.0       1420   \n",
      "3          0            107           0       0.0       1963   \n",
      "4          0            768           0       0.0       1786   \n",
      "\n",
      "   item_total_price  order_id  quantity         register_date  sales_channel  \\\n",
      "0            292.91     21804        10  2017-11-10T00:00:00Z              0   \n",
      "1            287.19      5486        20  2011-05-16T00:00:00Z              1   \n",
      "2            184.84     22662        12  2018-01-24T00:00:00Z              0   \n",
      "3            189.18      3956        18  2010-07-28T00:00:00Z              1   \n",
      "4             66.87      4730         5  2010-12-17T00:00:00Z              1   \n",
      "\n",
      "   segment_code  seller_code  total_price  unit_price  \n",
      "0             0          190      1613.53       25.04  \n",
      "1             5          153     11163.69       12.33  \n",
      "2             0          166      6432.12       12.80  \n",
      "3             0          156       831.82       10.51  \n",
      "4             0          218      1736.48       11.82  \n"
     ]
    }
   ],
   "source": [
    "#Reading and extracting dataset.\n",
    "reader = url.urlopen('https://github.com/totvslabs/datachallenge/raw/master/challenge.zip')\n",
    "file = ZipFile(StringIO(reader.read()))\n",
    "json_file = file.open('challenge.json')\n",
    "\n",
    "#Creating dataframe with dataset.\n",
    "df = pd.read_json(json_file)\n",
    "\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The first step in the preprocessing is to cast the date objects into datetimes using Pandas in order to allow for accessing the 'days' attribute. Then, the dates are transformed into days as explained before.\n",
    "\n",
    "In the following, the unused variables are dropped in order to remove useless calculations in the feature extraction step.\n",
    "\n",
    "The column 'is_churn' is the only column with missing values and it is used as the label of the classes (or dependent variable Y), so it is a binary classification and the mean strategy would not be suitable because it would create a third class.\n",
    "\n",
    "The most frequent strategy is adopted due to the imbalance between the classes (over 80% of the samples are from class 0, or clients that did not churn), what makes the probability of filling in the values correctly extremely high.\n",
    "\n",
    "Another possibility would be using clustering on the other features of the vectors with non-missing values, so as to create two cluster and, then, predict in which cluster the vectors with missing values are. This information could be used as the value missing. But this would require handling the categorical features, normalizing all features and finding an appropriate clutering method. And this task of finding a suitable method could even involve developing a specific distance metric, since the categorical features do not lay in an Euclidean space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: [u'is_churn']\n",
      "\n",
      "Original data\n",
      "--------------\n",
      "Classes: [ 0.  1. nan]\n",
      "Proportion of the class 0: 80.13\n",
      "Proportion of the class 1: 18.94\n",
      "Proportion of missing values: 0.94\n",
      "\n",
      "After handling missing data\n",
      "-----------------------------\n",
      "Classes: [0. 1.]\n",
      "Proportion of the class 0 : 81.06\n",
      "Proportion of the class 1 : 18.94\n"
     ]
    }
   ],
   "source": [
    "#Handling datetime data format.\n",
    "df['register_date'] = pd.to_datetime(df['register_date'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "extract_days_since_register_date(df, 'register_date')\n",
    "\n",
    "#Dropping unused variables in order to remove useless calculations in the feature extraction step.\n",
    "df = df.drop(['branch_id', 'seller_code', 'item_total_price', 'register_date'], axis=1)\n",
    "\n",
    "#Columns with missing values\n",
    "print 'Columns with missing values: ' + str(df.columns.to_numpy()[np.any(np.isnan(df.to_numpy()), axis=0)])\n",
    "\n",
    "#Classes imbalance\n",
    "print '\\nOriginal data\\n--------------'\n",
    "print 'Classes: ' + str(df['is_churn'].unique())\n",
    "print 'Proportion of the class 0: %.2f' % (100.0 * len(df['is_churn'][df['is_churn'] == 0.0]) / len(df['is_churn']))\n",
    "print 'Proportion of the class 1: %.2f' % (100.0 * len(df['is_churn'][df['is_churn'] == 1.0]) / len(df['is_churn']))\n",
    "print 'Proportion of missing values: %.2f' % (100.0 * len(df['is_churn'][df['is_churn'].isna()]) / len(df['is_churn']))\n",
    "\n",
    "#Filling in missing values.\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "df.loc[:,:] = imputer.fit_transform(df)\n",
    "\n",
    "print '\\nAfter handling missing data\\n-----------------------------'\n",
    "print 'Classes: ' + str(df['is_churn'].unique())\n",
    "print 'Proportion of the class 0 : %.2f' % (100.0 * len(df['is_churn'][df['is_churn'] == 0.0]) / len(df['is_churn']))\n",
    "print 'Proportion of the class 1 : %.2f' % (100.0 * len(df['is_churn'][df['is_churn'] == 1.0]) / len(df['is_churn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Here the function defined above is simply called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_churn  segment_code  group_code  average_unit_price  \\\n",
      "0       0.0           0.0         0.0           67.850670   \n",
      "1       0.0           0.0         0.0           54.524862   \n",
      "2       0.0           0.0         0.0           48.270912   \n",
      "3       0.0           0.0         0.0           78.777307   \n",
      "4       0.0           0.0         0.0           39.980399   \n",
      "\n",
      "   average_quantity_by_order  average_total_price_by_order  max_unit_price  \\\n",
      "0                  87.684211                  41351.434737          641.45   \n",
      "1                  76.595238                  25800.491429          372.51   \n",
      "2                  82.153846                  21577.883077          240.88   \n",
      "3                 160.698413                  80996.606190         4876.80   \n",
      "4                 106.902439                  26264.655122          615.67   \n",
      "\n",
      "   max_quantity_by_order  max_total_price_by_order  total_orders  \\\n",
      "0                  186.0                  89396.85            19   \n",
      "1                  167.0                  75869.43            42   \n",
      "2                  179.0                  74454.12            39   \n",
      "3                  481.0                 576592.40            63   \n",
      "4                  287.0                 133730.94            41   \n",
      "\n",
      "   days_since_last_order  days_since_first_order  order_frequency  \n",
      "0                  364.0                  3909.0         0.535815  \n",
      "1                  452.0                  4119.0         1.145038  \n",
      "2                  344.0                  4154.0         1.023353  \n",
      "3                  395.0                  4117.0         1.692184  \n",
      "4                  505.0                  4122.0         1.133223  \n"
     ]
    }
   ],
   "source": [
    "#Feature extraction.\n",
    "X = compute_features(df)\n",
    "\n",
    "print X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the names of the features are kept before casting the dataframe into a numpy array. The names will be useful later to show to the user what is the name of the feature that most contributed to the result predicted for each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features\n",
      "---------\n",
      "0 : segment code\n",
      "1 : group code\n",
      "2 : average unit price\n",
      "3 : average quantity by order\n",
      "4 : average total price by order\n",
      "5 : max unit price\n",
      "6 : max quantity by order\n",
      "7 : max total price by order\n",
      "8 : total orders\n",
      "9 : days since last order\n",
      "10 : days since first order\n",
      "11 : order frequency\n"
     ]
    }
   ],
   "source": [
    "#Keep feature names to indentify the likely reasons later on. Exclude 'is_churn' from the list.\n",
    "feature_names = [str(name).replace('_', ' ') for name in X.columns.tolist()][1:]\n",
    "\n",
    "print 'Features\\n---------'\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    print ('%d : %s') % (i, feature_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "The dataset is split in train and test sets before hyper parameters optimization. It was chosen to hold out 25% of the samples to evaluate the model, once the total number of samples is not very large. So, 25% lefts a good percentage of the samples for training the model and also allows for model evaluation. Due to the imbalance between the classes, the splitting is stratified so that the train and test sets have samples in the same proportion of each class. Otherwise, the splitting could result in one of the sets having very few samples of the positive class, harming the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (628L, 12L)\n",
      "Train labels shape: (628L,)\n",
      "Test data shape: (210L, 12L)\n",
      "Train labels shape: (210L,)\n"
     ]
    }
   ],
   "source": [
    "#Separate features and labels.\n",
    "y = X['is_churn'].to_numpy()\n",
    "X = X.drop(['is_churn'], axis=1).to_numpy()\n",
    "\n",
    "#Split train and test sets, 25% for test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)\n",
    "\n",
    "print 'Train data shape: ' + str(X_train.shape)\n",
    "print 'Train labels shape: ' + str(y_train.shape)\n",
    "print 'Test data shape: ' + str(X_test.shape)\n",
    "print 'Train labels shape: ' + str(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "The model chosen to predict customer churn is the GBM (Gradient Boosted Machines), specifically the XGBoost library was used, which is a optimized implementation of GBM. It was chosen because it is robust to feature normalization and can be used with categorical features provided that they are numeric. So, it is a model that does not require much preprocessing. Additionally, it can also return SHAP values, that expose the contribution of each feature for the values predicted. Therefore, it is also interpretable so that one can not only predict customer churn, but also can tell which is the likely reason for the customer staying or churning.\n",
    "\n",
    "An alternative methodology for model selection would be to use nested cross-validation with some different models and then choosing the best one. But it is more time consuming and using some types of models would require to handle categorical data (e.g., using one-hot encoding) and feature normalization. Furthermore, some models are not as interpretable as the XGBoost library made the GBM model to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate model.\n",
    "xgb_model = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters Optimization\n",
    "In order to tune the hyper parameters, grid search with 5-fold Cross-validation was used on the train set. The cross-validation is very useful here because the dataset resulted in a very limited number of samples of customer churn so that splitting them into three sets (train, validation and test) could eventually cause the model to underfit.\n",
    "\n",
    "It is important to highlight that when the 'cv' parameter is an integer, the folds splitting is stratified by default in the scikit-learn library. Also the refit parameter was set to 'True' in order to refit a model using the best parameters and all training data after applying the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.952\n",
      "\n",
      "Best params\n",
      "-------------\n",
      "colsample_bytree : 0.7\n",
      "silent : 1\n",
      "learning_rate : 0.005\n",
      "nthread : 4\n",
      "min_child_weight : 9\n",
      "n_estimators : 1000\n",
      "subsample : 0.8\n",
      "seed : 0\n",
      "objective : binary:hinge\n",
      "max_depth : 7\n"
     ]
    }
   ],
   "source": [
    "#Hyper parameters optimization through grid search with 5-fold cross-validation.\n",
    "parameters = {'nthread':[4],\n",
    "              'objective':['binary:hinge', 'binary:logistic'],\n",
    "              'learning_rate': [0.005, 0.05],\n",
    "              'max_depth': [3, 5, 7],\n",
    "              'min_child_weight': [9, 11],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.6, 0.7, 0.8],\n",
    "              'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "              'n_estimators': [1000],\n",
    "              'seed': [0]}\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters, n_jobs=4, \n",
    "                   cv=5,\n",
    "                   scoring=None,\n",
    "                   verbose=0, refit=True)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print 'Best score: %.3f' % clf.best_score_\n",
    "print '\\nBest params\\n-------------'\n",
    "for param, value in clf.best_params_.iteritems():\n",
    "    print ('%s : %s') % (param, str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Churn Prediction\n",
    "The best model refitted is then used to predict customer churn. Additionaly, the contributions of each feature to the predictions are computed and mapped to the feature names in order to tell which is the likely reason for each customer to stay or churn. The features with highest and lowest SHAP values are the ones that most influenced the prediction towards the class 1 (customer churn) and class 0 (customer stay), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer 0 will probably stay.\n",
      "\tLikely reason:        days since last order = 343.000\t(margin contribution): -0.181\n",
      "Customer 1 will probably stay.\n",
      "\tLikely reason:       days since first order = 1960.000\t(margin contribution): -0.173\n",
      "Customer 2 will probably stay.\n",
      "\tLikely reason:        days since last order = 351.000\t(margin contribution): -0.282\n",
      "Customer 3 will probably stay.\n",
      "\tLikely reason:       days since first order = 1962.000\t(margin contribution): -0.200\n",
      "Customer 4 will probably stay.\n",
      "\tLikely reason:       days since first order = 3241.000\t(margin contribution): -0.229\n",
      "Customer 5 will probably stay.\n",
      "\tLikely reason:       days since first order = 3903.000\t(margin contribution): -0.216\n",
      "Customer 6 will probably churn.\n",
      "\tLikely reason:              order frequency =   6.096\t(margin contribution): 1.261\n",
      "Customer 7 will probably stay.\n",
      "\tLikely reason:       days since first order = 2321.000\t(margin contribution): -0.223\n",
      "Customer 8 will probably stay.\n",
      "\tLikely reason:        days since last order = 359.000\t(margin contribution): -0.257\n",
      "Customer 9 will probably stay.\n",
      "\tLikely reason:                 segment code =   6.000\t(margin contribution): -0.253\n",
      "Customer 10 will probably stay.\n",
      "\tLikely reason:       days since first order = 2774.000\t(margin contribution): -0.194\n",
      "Customer 11 will probably stay.\n",
      "\tLikely reason:        days since last order = 375.000\t(margin contribution): -0.348\n",
      "Customer 12 will probably stay.\n",
      "\tLikely reason:        days since last order = 413.000\t(margin contribution): -0.261\n",
      "Customer 13 will probably stay.\n",
      "\tLikely reason:        days since last order = 409.000\t(margin contribution): -0.156\n",
      "Customer 14 will probably stay.\n",
      "\tLikely reason:       days since first order = 1893.000\t(margin contribution): -0.145\n",
      "Customer 15 will probably churn.\n",
      "\tLikely reason:       days since first order = 668.000\t(margin contribution): 0.971\n",
      "Customer 16 will probably stay.\n",
      "\tLikely reason:        max quantity by order = 196.000\t(margin contribution): -0.140\n",
      "Customer 17 will probably stay.\n",
      "\tLikely reason:        days since last order = 360.000\t(margin contribution): -0.249\n",
      "Customer 18 will probably stay.\n",
      "\tLikely reason:        days since last order = 361.000\t(margin contribution): -0.281\n",
      "Customer 19 will probably stay.\n",
      "\tLikely reason:       days since first order = 4153.000\t(margin contribution): -0.200\n",
      "Customer 20 will probably stay.\n",
      "\tLikely reason:        days since last order = 386.000\t(margin contribution): -0.191\n",
      "Customer 21 will probably stay.\n",
      "\tLikely reason:        days since last order = 359.000\t(margin contribution): -0.286\n",
      "Customer 22 will probably stay.\n",
      "\tLikely reason:       days since first order = 4165.000\t(margin contribution): -0.175\n",
      "Customer 23 will probably stay.\n",
      "\tLikely reason:        days since last order = 388.000\t(margin contribution): -0.171\n",
      "Customer 24 will probably churn.\n",
      "\tLikely reason:       days since first order = 889.000\t(margin contribution): 0.918\n",
      "Customer 25 will probably stay.\n",
      "\tLikely reason:       days since first order = 4088.000\t(margin contribution): -0.212\n",
      "Customer 26 will probably stay.\n",
      "\tLikely reason:        days since last order = 344.000\t(margin contribution): -0.290\n",
      "Customer 27 will probably stay.\n",
      "\tLikely reason:        days since last order = 354.000\t(margin contribution): -0.269\n",
      "Customer 28 will probably stay.\n",
      "\tLikely reason:        days since last order = 380.000\t(margin contribution): -0.223\n",
      "Customer 29 will probably stay.\n",
      "\tLikely reason:       days since first order = 4127.000\t(margin contribution): -0.159\n",
      "Customer 30 will probably churn.\n",
      "\tLikely reason:              order frequency =   4.073\t(margin contribution): 1.054\n",
      "Customer 31 will probably stay.\n",
      "\tLikely reason:        days since last order = 343.000\t(margin contribution): -0.247\n",
      "Customer 32 will probably stay.\n",
      "\tLikely reason:        days since last order = 387.000\t(margin contribution): -0.173\n",
      "Customer 33 will probably stay.\n",
      "\tLikely reason:        days since last order = 364.000\t(margin contribution): -0.267\n",
      "Customer 34 will probably stay.\n",
      "\tLikely reason:       days since first order = 2818.000\t(margin contribution): -0.211\n",
      "Customer 35 will probably churn.\n",
      "\tLikely reason:       days since first order = 575.000\t(margin contribution): 1.031\n",
      "Customer 36 will probably stay.\n",
      "\tLikely reason:       days since first order = 2598.000\t(margin contribution): -0.171\n",
      "Customer 37 will probably stay.\n",
      "\tLikely reason:       days since first order = 3643.000\t(margin contribution): -0.174\n",
      "Customer 38 will probably churn.\n",
      "\tLikely reason:              order frequency =  12.613\t(margin contribution): 1.188\n",
      "Customer 39 will probably stay.\n",
      "\tLikely reason:       days since first order = 3153.000\t(margin contribution): -0.187\n",
      "Customer 40 will probably stay.\n",
      "\tLikely reason:       days since first order = 4008.000\t(margin contribution): -0.183\n",
      "Customer 41 will probably stay.\n",
      "\tLikely reason:              order frequency =   0.829\t(margin contribution): -0.168\n",
      "Customer 42 will probably stay.\n",
      "\tLikely reason:        days since last order = 338.000\t(margin contribution): -0.225\n",
      "Customer 43 will probably stay.\n",
      "\tLikely reason:                 segment code =   7.000\t(margin contribution): -0.331\n",
      "Customer 44 will probably stay.\n",
      "\tLikely reason:       days since first order = 3581.000\t(margin contribution): -0.201\n",
      "Customer 45 will probably stay.\n",
      "\tLikely reason:       days since first order = 4131.000\t(margin contribution): -0.157\n",
      "Customer 46 will probably stay.\n",
      "\tLikely reason:       days since first order = 2600.000\t(margin contribution): -0.162\n",
      "Customer 47 will probably stay.\n",
      "\tLikely reason:        days since last order = 352.000\t(margin contribution): -0.267\n",
      "Customer 48 will probably stay.\n",
      "\tLikely reason:       days since first order = 4106.000\t(margin contribution): -0.249\n",
      "Customer 49 will probably stay.\n",
      "\tLikely reason:       days since first order = 4134.000\t(margin contribution): -0.192\n",
      "Customer 50 will probably stay.\n",
      "\tLikely reason:        days since last order = 345.000\t(margin contribution): -0.267\n",
      "Customer 51 will probably stay.\n",
      "\tLikely reason:       days since first order = 4169.000\t(margin contribution): -0.182\n",
      "Customer 52 will probably stay.\n",
      "\tLikely reason:       days since first order = 3974.000\t(margin contribution): -0.206\n",
      "Customer 53 will probably stay.\n",
      "\tLikely reason:        days since last order = 361.000\t(margin contribution): -0.277\n",
      "Customer 54 will probably stay.\n",
      "\tLikely reason:       days since first order = 3985.000\t(margin contribution): -0.212\n",
      "Customer 55 will probably stay.\n",
      "\tLikely reason:       days since first order = 2478.000\t(margin contribution): -0.221\n",
      "Customer 56 will probably churn.\n",
      "\tLikely reason:       days since first order = 745.000\t(margin contribution): 1.165\n",
      "Customer 57 will probably stay.\n",
      "\tLikely reason:                 segment code =   7.000\t(margin contribution): -0.384\n",
      "Customer 58 will probably stay.\n",
      "\tLikely reason:        days since last order = 358.000\t(margin contribution): -0.541\n",
      "Customer 59 will probably stay.\n",
      "\tLikely reason:       days since first order = 4068.000\t(margin contribution): -0.182\n",
      "Customer 60 will probably stay.\n",
      "\tLikely reason:        days since last order = 365.000\t(margin contribution): -0.289\n",
      "Customer 61 will probably stay.\n",
      "\tLikely reason:        days since last order = 360.000\t(margin contribution): -0.272\n",
      "Customer 62 will probably stay.\n",
      "\tLikely reason:       days since first order = 3790.000\t(margin contribution): -0.232\n",
      "Customer 63 will probably stay.\n",
      "\tLikely reason:       days since first order = 4118.000\t(margin contribution): -0.204\n",
      "Customer 64 will probably stay.\n",
      "\tLikely reason:       days since first order = 3847.000\t(margin contribution): -0.206\n",
      "Customer 65 will probably churn.\n",
      "\tLikely reason:              order frequency =   4.716\t(margin contribution): 1.214\n",
      "Customer 66 will probably stay.\n",
      "\tLikely reason:        days since last order = 350.000\t(margin contribution): -0.258\n",
      "Customer 67 will probably stay.\n",
      "\tLikely reason:        days since last order = 359.000\t(margin contribution): -0.276\n",
      "Customer 68 will probably stay.\n",
      "\tLikely reason:       days since first order = 2514.000\t(margin contribution): -0.210\n",
      "Customer 69 will probably stay.\n",
      "\tLikely reason:       days since first order = 4132.000\t(margin contribution): -0.207\n",
      "Customer 70 will probably stay.\n",
      "\tLikely reason:        days since last order = 373.000\t(margin contribution): -0.254\n",
      "Customer 71 will probably stay.\n",
      "\tLikely reason:        days since last order = 380.000\t(margin contribution): -0.270\n",
      "Customer 72 will probably stay.\n",
      "\tLikely reason:       days since first order = 2746.000\t(margin contribution): -0.157\n",
      "Customer 73 will probably stay.\n",
      "\tLikely reason:       days since first order = 4096.000\t(margin contribution): -0.253\n",
      "Customer 74 will probably stay.\n",
      "\tLikely reason:        days since last order = 361.000\t(margin contribution): -0.264\n",
      "Customer 75 will probably stay.\n",
      "\tLikely reason:       days since first order = 4020.000\t(margin contribution): -0.237\n",
      "Customer 76 will probably stay.\n",
      "\tLikely reason:        days since last order = 354.000\t(margin contribution): -0.284\n",
      "Customer 77 will probably stay.\n",
      "\tLikely reason:       days since first order = 4113.000\t(margin contribution): -0.206\n",
      "Customer 78 will probably stay.\n",
      "\tLikely reason:       days since first order = 2419.000\t(margin contribution): -0.183\n",
      "Customer 79 will probably stay.\n",
      "\tLikely reason:       days since first order = 4125.000\t(margin contribution): -0.207\n",
      "Customer 80 will probably churn.\n",
      "\tLikely reason:       days since first order = 557.000\t(margin contribution): 0.851\n",
      "Customer 81 will probably stay.\n",
      "\tLikely reason:        days since last order = 340.000\t(margin contribution): -0.151\n",
      "Customer 82 will probably stay.\n",
      "\tLikely reason:                   group code =   2.000\t(margin contribution): -0.161\n",
      "Customer 83 will probably stay.\n",
      "\tLikely reason:       days since first order = 2026.000\t(margin contribution): -0.156\n",
      "Customer 84 will probably stay.\n",
      "\tLikely reason:        days since last order = 388.000\t(margin contribution): -0.218\n",
      "Customer 85 will probably stay.\n",
      "\tLikely reason:       days since first order = 3458.000\t(margin contribution): -0.171\n",
      "Customer 86 will probably stay.\n",
      "\tLikely reason:        days since last order = 351.000\t(margin contribution): -0.277\n",
      "Customer 87 will probably stay.\n",
      "\tLikely reason:       days since first order = 4132.000\t(margin contribution): -0.222\n",
      "Customer 88 will probably stay.\n",
      "\tLikely reason:        days since last order = 375.000\t(margin contribution): -0.217\n",
      "Customer 89 will probably stay.\n",
      "\tLikely reason:       days since first order = 4050.000\t(margin contribution): -0.205\n",
      "Customer 90 will probably stay.\n",
      "\tLikely reason:        days since last order = 364.000\t(margin contribution): -0.277\n",
      "Customer 91 will probably stay.\n",
      "\tLikely reason:        days since last order = 411.000\t(margin contribution): -0.188\n",
      "Customer 92 will probably stay.\n",
      "\tLikely reason:        days since last order = 361.000\t(margin contribution): -0.263\n",
      "Customer 93 will probably stay.\n",
      "\tLikely reason:       days since first order = 4061.000\t(margin contribution): -0.233\n",
      "Customer 94 will probably stay.\n",
      "\tLikely reason:        days since last order = 381.000\t(margin contribution): -0.212\n",
      "Customer 95 will probably churn.\n",
      "\tLikely reason:              order frequency =   5.691\t(margin contribution): 1.237\n",
      "Customer 96 will probably stay.\n",
      "\tLikely reason:              order frequency =   1.198\t(margin contribution): -0.175\n",
      "Customer 97 will probably stay.\n",
      "\tLikely reason:        days since last order = 367.000\t(margin contribution): -0.277\n",
      "Customer 98 will probably stay.\n",
      "\tLikely reason:       days since first order = 1390.000\t(margin contribution): -0.152\n",
      "Customer 99 will probably stay.\n",
      "\tLikely reason:        days since last order = 381.000\t(margin contribution): -0.244\n",
      "Customer 100 will probably churn.\n",
      "\tLikely reason:       days since first order = 577.000\t(margin contribution): 1.039\n",
      "Customer 101 will probably stay.\n",
      "\tLikely reason:        days since last order = 351.000\t(margin contribution): -0.279\n",
      "Customer 102 will probably stay.\n",
      "\tLikely reason:        days since last order = 386.000\t(margin contribution): -0.197\n",
      "Customer 103 will probably stay.\n",
      "\tLikely reason:        days since last order = 375.000\t(margin contribution): -0.217\n",
      "Customer 104 will probably stay.\n",
      "\tLikely reason:        days since last order = 353.000\t(margin contribution): -0.286\n",
      "Customer 105 will probably stay.\n",
      "\tLikely reason:        days since last order = 399.000\t(margin contribution): -0.177\n",
      "Customer 106 will probably stay.\n",
      "\tLikely reason:        days since last order = 353.000\t(margin contribution): -0.270\n",
      "Customer 107 will probably stay.\n",
      "\tLikely reason:       days since first order = 1848.000\t(margin contribution): -0.166\n",
      "Customer 108 will probably stay.\n",
      "\tLikely reason:        days since last order = 388.000\t(margin contribution): -0.278\n",
      "Customer 109 will probably stay.\n",
      "\tLikely reason:        days since last order = 403.000\t(margin contribution): -0.148\n",
      "Customer 110 will probably stay.\n",
      "\tLikely reason:        days since last order = 346.000\t(margin contribution): -0.385\n",
      "Customer 111 will probably stay.\n",
      "\tLikely reason:        days since last order = 359.000\t(margin contribution): -0.279\n",
      "Customer 112 will probably stay.\n",
      "\tLikely reason:       days since first order = 1418.000\t(margin contribution): -0.177\n",
      "Customer 113 will probably stay.\n",
      "\tLikely reason:        days since last order = 387.000\t(margin contribution): -0.199\n",
      "Customer 114 will probably stay.\n",
      "\tLikely reason:       days since first order = 4169.000\t(margin contribution): -0.210\n",
      "Customer 115 will probably stay.\n",
      "\tLikely reason:        days since last order = 389.000\t(margin contribution): -0.180\n",
      "Customer 116 will probably stay.\n",
      "\tLikely reason:       days since first order = 1936.000\t(margin contribution): -0.205\n",
      "Customer 117 will probably stay.\n",
      "\tLikely reason:        days since last order = 408.000\t(margin contribution): -0.177\n",
      "Customer 118 will probably stay.\n",
      "\tLikely reason:       days since first order = 4134.000\t(margin contribution): -0.260\n",
      "Customer 119 will probably stay.\n",
      "\tLikely reason:        days since last order = 358.000\t(margin contribution): -0.262\n",
      "Customer 120 will probably stay.\n",
      "\tLikely reason:        days since last order = 353.000\t(margin contribution): -0.221\n",
      "Customer 121 will probably stay.\n",
      "\tLikely reason:       days since first order = 3636.000\t(margin contribution): -0.209\n",
      "Customer 122 will probably stay.\n",
      "\tLikely reason:        days since last order = 344.000\t(margin contribution): -0.490\n",
      "Customer 123 will probably stay.\n",
      "\tLikely reason:        days since last order = 350.000\t(margin contribution): -0.435\n",
      "Customer 124 will probably stay.\n",
      "\tLikely reason:       days since first order = 1640.000\t(margin contribution): -0.173\n",
      "Customer 125 will probably stay.\n",
      "\tLikely reason:       days since first order = 4130.000\t(margin contribution): -0.212\n",
      "Customer 126 will probably stay.\n",
      "\tLikely reason:       days since first order = 4060.000\t(margin contribution): -0.158\n",
      "Customer 127 will probably stay.\n",
      "\tLikely reason:        days since last order = 399.000\t(margin contribution): -0.429\n",
      "Customer 128 will probably stay.\n",
      "\tLikely reason:       days since first order = 2853.000\t(margin contribution): -0.221\n",
      "Customer 129 will probably stay.\n",
      "\tLikely reason:        days since last order = 340.000\t(margin contribution): -0.304\n",
      "Customer 130 will probably stay.\n",
      "\tLikely reason:       days since first order = 4033.000\t(margin contribution): -0.232\n",
      "Customer 131 will probably stay.\n",
      "\tLikely reason:       days since first order = 3974.000\t(margin contribution): -0.199\n",
      "Customer 132 will probably stay.\n",
      "\tLikely reason:        days since last order = 359.000\t(margin contribution): -0.300\n",
      "Customer 133 will probably stay.\n",
      "\tLikely reason:       days since first order = 3859.000\t(margin contribution): -0.178\n",
      "Customer 134 will probably stay.\n",
      "\tLikely reason:       days since first order = 1337.000\t(margin contribution): -0.208\n",
      "Customer 135 will probably stay.\n",
      "\tLikely reason:        days since last order = 345.000\t(margin contribution): -0.287\n",
      "Customer 136 will probably stay.\n",
      "\tLikely reason:       days since first order = 4102.000\t(margin contribution): -0.206\n",
      "Customer 137 will probably stay.\n",
      "\tLikely reason:       days since first order = 4103.000\t(margin contribution): -0.205\n",
      "Customer 138 will probably stay.\n",
      "\tLikely reason:        days since last order = 379.000\t(margin contribution): -0.247\n",
      "Customer 139 will probably stay.\n",
      "\tLikely reason:       days since first order = 3355.000\t(margin contribution): -0.162\n",
      "Customer 140 will probably stay.\n",
      "\tLikely reason:       days since first order = 3304.000\t(margin contribution): -0.195\n",
      "Customer 141 will probably churn.\n",
      "\tLikely reason:       days since first order = 793.000\t(margin contribution): 1.119\n",
      "Customer 142 will probably stay.\n",
      "\tLikely reason:                 segment code =   7.000\t(margin contribution): -0.326\n",
      "Customer 143 will probably stay.\n",
      "\tLikely reason:       days since first order = 4096.000\t(margin contribution): -0.237\n",
      "Customer 144 will probably churn.\n",
      "\tLikely reason:        days since last order = 491.000\t(margin contribution): 0.647\n",
      "Customer 145 will probably stay.\n",
      "\tLikely reason:        days since last order = 360.000\t(margin contribution): -0.289\n",
      "Customer 146 will probably stay.\n",
      "\tLikely reason:        days since last order = 380.000\t(margin contribution): -0.234\n",
      "Customer 147 will probably stay.\n",
      "\tLikely reason:        days since last order = 373.000\t(margin contribution): -0.360\n",
      "Customer 148 will probably stay.\n",
      "\tLikely reason:        days since last order = 343.000\t(margin contribution): -0.150\n",
      "Customer 149 will probably stay.\n",
      "\tLikely reason:        days since last order = 381.000\t(margin contribution): -0.216\n",
      "Customer 150 will probably stay.\n",
      "\tLikely reason:       days since first order = 1577.000\t(margin contribution): -0.161\n",
      "Customer 151 will probably stay.\n",
      "\tLikely reason:        days since last order = 406.000\t(margin contribution): -0.174\n",
      "Customer 152 will probably stay.\n",
      "\tLikely reason:       days since first order = 4118.000\t(margin contribution): -0.199\n",
      "Customer 153 will probably stay.\n",
      "\tLikely reason:       days since first order = 1724.000\t(margin contribution): -0.149\n",
      "Customer 154 will probably stay.\n",
      "\tLikely reason:       days since first order = 3615.000\t(margin contribution): -0.152\n",
      "Customer 155 will probably stay.\n",
      "\tLikely reason:        days since last order = 394.000\t(margin contribution): -0.175\n",
      "Customer 156 will probably stay.\n",
      "\tLikely reason:       days since first order = 4085.000\t(margin contribution): -0.184\n",
      "Customer 157 will probably stay.\n",
      "\tLikely reason:       days since first order = 2471.000\t(margin contribution): -0.270\n",
      "Customer 158 will probably stay.\n",
      "\tLikely reason:        days since last order = 375.000\t(margin contribution): -0.211\n",
      "Customer 159 will probably stay.\n",
      "\tLikely reason:           average unit price =  52.264\t(margin contribution): -0.104\n",
      "Customer 160 will probably stay.\n",
      "\tLikely reason:       days since first order = 4036.000\t(margin contribution): -0.204\n",
      "Customer 161 will probably stay.\n",
      "\tLikely reason:       days since first order = 3847.000\t(margin contribution): -0.197\n",
      "Customer 162 will probably stay.\n",
      "\tLikely reason:       days since first order = 4088.000\t(margin contribution): -0.214\n",
      "Customer 163 will probably stay.\n",
      "\tLikely reason:        days since last order = 350.000\t(margin contribution): -0.267\n",
      "Customer 164 will probably stay.\n",
      "\tLikely reason:              order frequency =   0.995\t(margin contribution): -0.192\n",
      "Customer 165 will probably stay.\n",
      "\tLikely reason:        days since last order = 401.000\t(margin contribution): -0.162\n",
      "Customer 166 will probably stay.\n",
      "\tLikely reason:        days since last order = 353.000\t(margin contribution): -0.271\n",
      "Customer 167 will probably stay.\n",
      "\tLikely reason:       days since first order = 3145.000\t(margin contribution): -0.169\n",
      "Customer 168 will probably churn.\n",
      "\tLikely reason:              order frequency =   5.533\t(margin contribution): 1.129\n",
      "Customer 169 will probably stay.\n",
      "\tLikely reason:       days since first order = 1191.000\t(margin contribution): -0.153\n",
      "Customer 170 will probably stay.\n",
      "\tLikely reason:        days since last order = 395.000\t(margin contribution): -0.183\n",
      "Customer 171 will probably stay.\n",
      "\tLikely reason:        days since last order = 344.000\t(margin contribution): -0.214\n",
      "Customer 172 will probably stay.\n",
      "\tLikely reason:        days since last order = 358.000\t(margin contribution): -0.253\n",
      "Customer 173 will probably stay.\n",
      "\tLikely reason:       days since first order = 4124.000\t(margin contribution): -0.159\n",
      "Customer 174 will probably stay.\n",
      "\tLikely reason:       days since first order = 4098.000\t(margin contribution): -0.180\n",
      "Customer 175 will probably stay.\n",
      "\tLikely reason:        days since last order = 350.000\t(margin contribution): -0.286\n",
      "Customer 176 will probably stay.\n",
      "\tLikely reason:        days since last order = 373.000\t(margin contribution): -0.285\n",
      "Customer 177 will probably churn.\n",
      "\tLikely reason:       days since first order = 626.000\t(margin contribution): 0.758\n",
      "Customer 178 will probably stay.\n",
      "\tLikely reason:       days since first order = 3200.000\t(margin contribution): -0.239\n",
      "Customer 179 will probably stay.\n",
      "\tLikely reason:           average unit price =  36.682\t(margin contribution): -0.153\n",
      "Customer 180 will probably stay.\n",
      "\tLikely reason:        days since last order = 351.000\t(margin contribution): -0.265\n",
      "Customer 181 will probably stay.\n",
      "\tLikely reason:        days since last order = 417.000\t(margin contribution): -0.256\n",
      "Customer 182 will probably stay.\n",
      "\tLikely reason:        days since last order = 350.000\t(margin contribution): -0.463\n",
      "Customer 183 will probably stay.\n",
      "\tLikely reason:        days since last order = 361.000\t(margin contribution): -0.227\n",
      "Customer 184 will probably stay.\n",
      "\tLikely reason:       days since first order = 4081.000\t(margin contribution): -0.208\n",
      "Customer 185 will probably stay.\n",
      "\tLikely reason:        days since last order = 413.000\t(margin contribution): -0.345\n",
      "Customer 186 will probably churn.\n",
      "\tLikely reason:       days since first order = 801.000\t(margin contribution): 0.917\n",
      "Customer 187 will probably stay.\n",
      "\tLikely reason:              order frequency =   1.062\t(margin contribution): -0.156\n",
      "Customer 188 will probably churn.\n",
      "\tLikely reason:       days since first order = 616.000\t(margin contribution): 0.783\n",
      "Customer 189 will probably stay.\n",
      "\tLikely reason:       days since first order = 4112.000\t(margin contribution): -0.167\n",
      "Customer 190 will probably stay.\n",
      "\tLikely reason:       days since first order = 2201.000\t(margin contribution): -0.179\n",
      "Customer 191 will probably stay.\n",
      "\tLikely reason:       days since first order = 4026.000\t(margin contribution): -0.209\n",
      "Customer 192 will probably churn.\n",
      "\tLikely reason:       days since first order = 645.000\t(margin contribution): 0.844\n",
      "Customer 193 will probably stay.\n",
      "\tLikely reason:        days since last order = 406.000\t(margin contribution): -0.168\n",
      "Customer 194 will probably stay.\n",
      "\tLikely reason:        days since last order = 352.000\t(margin contribution): -0.498\n",
      "Customer 195 will probably stay.\n",
      "\tLikely reason:       days since first order = 4060.000\t(margin contribution): -0.180\n",
      "Customer 196 will probably stay.\n",
      "\tLikely reason:        days since last order = 351.000\t(margin contribution): -0.285\n",
      "Customer 197 will probably stay.\n",
      "\tLikely reason:       days since first order = 4126.000\t(margin contribution): -0.216\n",
      "Customer 198 will probably stay.\n",
      "\tLikely reason:        days since last order = 361.000\t(margin contribution): -0.273\n",
      "Customer 199 will probably stay.\n",
      "\tLikely reason:       days since first order = 4154.000\t(margin contribution): -0.202\n",
      "Customer 200 will probably stay.\n",
      "\tLikely reason:        days since last order = 381.000\t(margin contribution): -0.230\n",
      "Customer 201 will probably stay.\n",
      "\tLikely reason:        days since last order = 403.000\t(margin contribution): -0.153\n",
      "Customer 202 will probably stay.\n",
      "\tLikely reason:        days since last order = 387.000\t(margin contribution): -0.197\n",
      "Customer 203 will probably stay.\n",
      "\tLikely reason:       days since first order = 3999.000\t(margin contribution): -0.217\n",
      "Customer 204 will probably stay.\n",
      "\tLikely reason:              order frequency =   1.006\t(margin contribution): -0.159\n",
      "Customer 205 will probably stay.\n",
      "\tLikely reason:       days since first order = 4103.000\t(margin contribution): -0.228\n",
      "Customer 206 will probably stay.\n",
      "\tLikely reason:        days since last order = 408.000\t(margin contribution): -0.165\n",
      "Customer 207 will probably stay.\n",
      "\tLikely reason:        days since last order = 352.000\t(margin contribution): -0.277\n",
      "Customer 208 will probably stay.\n",
      "\tLikely reason:        days since last order = 409.000\t(margin contribution): -0.349\n",
      "Customer 209 will probably stay.\n",
      "\tLikely reason:        days since last order = 360.000\t(margin contribution): -0.280\n"
     ]
    }
   ],
   "source": [
    "#Use best model found to predict churn.\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#Get and report likely reason.\n",
    "xgb_booster = clf.best_estimator_.get_booster()\n",
    "contrib = xgb_booster.predict(xgb.DMatrix(X_test), pred_contribs=True)\n",
    "\n",
    "for i, pred in enumerate(y_pred):\n",
    "    if pred == 1.0:\n",
    "        most_influential_reason_idx = np.argmax(contrib[i,:-1]) #the feature that most influenced positively\n",
    "        client_status = 'churn'\n",
    "    else:\n",
    "        most_influential_reason_idx = np.argmin(contrib[i,:-1]) #the feature that most influenced negatively\n",
    "        client_status = 'stay'\n",
    "    print ('Customer %d will probably %s.\\n\\tLikely reason: %28s = %7s\\t(margin contribution): %.3f') %\\\n",
    "          (i,\n",
    "           client_status,\n",
    "           feature_names[most_influential_reason_idx],\n",
    "           '{:.3f}'.format(X_test[i, most_influential_reason_idx]),\n",
    "           contrib[i, most_influential_reason_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Cohen's Kappa Score and F1-score were chosen to assess the model. Cohen's Kappa Score is a measure of agreement between raters that takes into account the likelihood of they agreeing by chance. In this case, the score is measuring the agreement between the actual labels and the predicted labels. According to a possible interpretation of the score, a value above 0.8 may be regarded as 'almost perfect' [2]. Additionally, the F1-score is used because it combines the values of precision and recall, which are very suitable to the problem being tackled: the precision shows how much a prediction of the model is reliable and the recall tells how many of the customers that will stay or churn are indentified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cohen Kappa Score: 0.942\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99       190\n",
      "         1.0       1.00      0.90      0.95        20\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       210\n",
      "   macro avg       0.99      0.95      0.97       210\n",
      "weighted avg       0.99      0.99      0.99       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Report results.\n",
    "print ('\\nCohen Kappa Score: %.3f\\n') % (cohen_kappa_score(y_test, y_pred))\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Landis, J. R. and Koch, G. G. (1977). The measurement of observer agreement for\n",
    "categorical data. Biometrics, 33(1):159--174. ISSN 0006-341X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4.2\n",
    "If I had more time, I would improve the model by thinking of a few more features and maybe working on an ensemble of different classifiers. Using an ensemble would allow me for training each classifier using just some features that are suitable to the model and then to use one type of majority vote approach to output the final prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
